# DRL-based Network Function Virtualization Placement

H·ªá th·ªëng s·ª≠ d·ª•ng Deep Reinforcement Learning (DQN + VAE) ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n ƒë·∫∑t Virtual Network Functions (VNFs) trong m√¥i tr∆∞·ªùng NFV/SDN.

## üìÅ C·∫•u tr√∫c th∆∞ m·ª•c
```
DRL-NFV/
‚îú‚îÄ‚îÄ agents/                          # C√°c agent DRL
‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.py                # Deep Q-Network agent
‚îÇ   ‚îú‚îÄ‚îÄ vae_agent.py                # Variational Autoencoder agent
‚îÇ   ‚îî‚îÄ‚îÄ vae_trainer.py              # Trainer cho VAE
‚îÇ
‚îú‚îÄ‚îÄ core/                            # Core logic c·ªßa simulator
‚îÇ   ‚îú‚îÄ‚îÄ dc.py                       # DataCenter v√† SwitchNode
‚îÇ   ‚îú‚îÄ‚îÄ request.py                  # SFC Request
‚îÇ   ‚îú‚îÄ‚îÄ sfc_manager.py              # Qu·∫£n l√Ω requests
‚îÇ   ‚îú‚îÄ‚îÄ simulator.py                # Discrete-event simulator
‚îÇ   ‚îú‚îÄ‚îÄ statistics.py               # Th·ªëng k√™ metrics
‚îÇ   ‚îú‚îÄ‚îÄ topology.py                 # Network topology manager
‚îÇ   ‚îî‚îÄ‚îÄ vnf.py                      # VNF Instance
‚îÇ
‚îú‚îÄ‚îÄ envs/                            # Gym Environment
‚îÇ   ‚îú‚îÄ‚îÄ action_handler.py           # X·ª≠ l√Ω actions (allocate/uninstall)
‚îÇ   ‚îú‚îÄ‚îÄ debug_tracker.py            # Debug v√† tracking
‚îÇ   ‚îú‚îÄ‚îÄ env.py                      # SFCEnvironment (Gym interface)
‚îÇ   ‚îú‚îÄ‚îÄ observer.py                 # State observation
‚îÇ   ‚îú‚îÄ‚îÄ request_selector.py         # Ch·ªçn request ∆∞u ti√™n
‚îÇ   ‚îú‚îÄ‚îÄ selectors.py                # DC ordering strategies
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                    # Action mask v√† utilities
‚îÇ
‚îú‚îÄ‚îÄ runners/                         # Training v√† testing pipelines
‚îÇ   ‚îú‚îÄ‚îÄ compare.py                  # So s√°nh DQN vs VAE-DQN
‚îÇ   ‚îú‚îÄ‚îÄ data_generator.py           # Generate random scenarios
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py              # Load t·ª´ JSON files
‚îÇ   ‚îú‚îÄ‚îÄ runner.py                   # Core Runner class
‚îÇ   ‚îú‚îÄ‚îÄ train_dqn.py                # Training DQN
‚îÇ   ‚îî‚îÄ‚îÄ train_vae.py                # Collect & train VAE
‚îÇ
‚îú‚îÄ‚îÄ data/                            # Test datasets (30+ files)
‚îÇ   ‚îú‚îÄ‚îÄ cogent_centers_atlanta_easy_s1.json
‚îÇ   ‚îú‚îÄ‚îÄ cogent_centers_atlanta_medium_s1.json
‚îÇ   ‚îú‚îÄ‚îÄ cogent_centers_atlanta_hard_s1.json
‚îÇ   ‚îî‚îÄ‚îÄ ...                         # More locations & difficulties
‚îÇ
‚îú‚îÄ‚îÄ models/                          # Saved models
‚îÇ   ‚îú‚îÄ‚îÄ best_model_q.weights.h5    # DQN model
‚îÇ   ‚îú‚îÄ‚îÄ vae_model_*.weights.h5      # VAE models
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint_*.weights.h5     # Checkpoints
‚îÇ
‚îú‚îÄ‚îÄ fig/                             # Output figures
‚îÇ   ‚îú‚îÄ‚îÄ comparison_grouped.png      # Grouped by location & difficulty
‚îÇ   ‚îú‚îÄ‚îÄ comparison_by_difficulty.png # By difficulty level
‚îÇ   ‚îî‚îÄ‚îÄ comparison_by_location.png  # By location
‚îÇ
‚îú‚îÄ‚îÄ config.py                        # Global configuration
‚îú‚îÄ‚îÄ main.py                          # CLI entry point
‚îî‚îÄ‚îÄ README.md
```

## üéØ √ù t∆∞·ªüng thu·∫≠t to√°n

### B√†i to√°n
ƒê·∫∑t c√°c Virtual Network Functions (VNFs) l√™n c√°c Data Centers ƒë·ªÉ ph·ª•c v·ª• Service Function Chain (SFC) requests v·ªõi m·ª•c ti√™u:
- **Maximize**: Acceptance Ratio (s·ªë requests ƒë∆∞·ª£c ph·ª•c v·ª•)
- **Minimize**: End-to-End Delay, Resource consumption
- **Constraints**: CPU, RAM, Storage, Bandwidth, Latency

### Ki·∫øn tr√∫c DRL

#### 1. Deep Q-Network (DQN)
- **State**: 3 inputs
  - DC State: `[CPU, RAM, installed_VNFs, idle_VNFs]`
  - DC-Demand State: `[VNF_demand, chain_patterns]`
  - Global State: `[total_requests, avg_delay, global_VNF_demand]`
  
- **Action Space**: `2V + 1` (V = MAX_VNF_TYPES = 10)
  - Action 0: WAIT
  - Actions 1‚Üí10: UNINSTALL VNF type 0-9
  - Actions 11‚Üí20: ALLOCATE VNF type 0-9

- **Reward**:
  - `+2.0`: SFC completed
  - `-1.5`: SFC dropped (timeout)
  - `-1.0`: Invalid action
  - `-0.5`: Uninstall needed VNF
  - `0.0`: Otherwise

- **Network Architecture**:
```
  Input1 [DC] ‚Üí Dense(32) ‚Üí
  Input2 [Demand] ‚Üí Dense(64) ‚Üí Concat ‚Üí Attention ‚Üí Dense(96) ‚Üí Dense(64) ‚Üí Q-values
  Input3 [Global] ‚Üí Dense(64) ‚Üí
```

#### 2. VAE-enhanced DQN
- **VAE Encoder**: DC_State ‚Üí Latent representation (32D)
- **VAE Decoder**: Latent ‚Üí Next_DC_State (prediction)
- **Value Network**: Latent ‚Üí DC priority score
- **Benefit**: DCs ƒë∆∞·ª£c s·∫Øp x·∫øp theo value t·ª´ VAE thay v√¨ heuristic priority

### Reconfigurability
- **Padding scheme**: State size c·ªë ƒë·ªãnh v·ªõi `MAX_VNF_TYPES=10`
- **Flexible training**: VNF types t·ª´ 2-10 trong m·ªói episode
- **No retraining needed**: Model ho·∫°t ƒë·ªông v·ªõi b·∫•t k·ª≥ s·ªë VNF types n√†o (2-10)

## üöÄ Pipeline ƒë·∫ßy ƒë·ªß

### Pipeline t·ª± ƒë·ªông (Khuy·∫øn ngh·ªã)
```bash
python main.py train pipeline --episodes 500 --vae-episodes 200
```

**Ch·ª©c nƒÉng:**
1. **Train DQN** v·ªõi random scenarios (500 episodes)
2. **Collect VAE data** t·ª´ DQN ƒë√£ train (200 episodes)
3. **Train VAE models** (Encoder, Decoder, Value Network)

**Output:**
```
================================================================================
FULL TRAINING PIPELINE
================================================================================
Step 1: Train DQN
Step 2: Collect VAE data
Step 3: Train VAE models
================================================================================

>>> STEP 1: Training DQN with random scenarios...
Episode 1/500 [DC:4 SW:14 VNF:6 Req:25]: R=450 AR=76.0%
...
Checkpoint 50: AR=85.34%
...

>>> STEP 2: Collecting VAE data using trained DQN...
Episode 10/200: 45230 samples
...

>>> Training VAE (234567 samples)...
    Epoch 5/50 - Loss: 0.4523
    ...

>>> Training Value Network (234567 samples)...
    Epoch 5/100 - Loss: 0.2134
    ...

================================================================================
PIPELINE COMPLETE!
  DQN model: models/best_model
  VAE model: models/vae_model
================================================================================
```

---

### Pipeline th·ªß c√¥ng (Optional)

#### Step 1: Train DQN
```bash
python main.py train dqn --episodes 400
```

#### Step 2: Train VAE
```bash
python main.py train vae --vae-episodes 150
```

---

### So s√°nh DQN vs VAE-DQN

#### Tr√™n t·∫•t c·∫£ files (M·∫∑c ƒë·ªãnh)
```bash
python main.py compare
```

**Ch·ª©c nƒÉng:**
- Test tr√™n **T·∫§T C·∫¢** files trong `data/` (30+ files)
- T√≠nh to√°n: Acceptance Ratio, E2E Delay, Throughput
- Ph√¢n t√≠ch theo:
  - **Location**
  - **Difficulty** (Easy, Medium, Hard)
  - **Combined** (Location + Difficulty)

**Output:**
```
================================================================================
Comparing DQN vs VAE-DQN on all files in data/
================================================================================

Testing 30 files...

File 1/30: cogent_centers_atlanta_easy_s1.json
  DQN:     AR=92.3% Delay=47.5ms TP=245.6
  VAE-DQN: AR=95.1% Delay=43.2ms TP=267.3

File 2/30: cogent_centers_atlanta_medium_s1.json
  DQN:     AR=89.7% Delay=51.3ms TP=223.4
  VAE-DQN: AR=93.4% Delay=46.8ms TP=251.2
...

================================================================================
OVERALL RESULTS (30 files)
================================================================================
DQN Average:
  Acceptance Ratio: 90.45%
  E2E Delay: 48.32ms
  Throughput: 234.56

VAE-DQN Average:
  Acceptance Ratio: 93.78%
  E2E Delay: 44.15ms
  Throughput: 256.78

Improvement:
  AR: +3.33%
  Delay: -8.63%
  Throughput: +9.48%

Plots saved:
  - fig/comparison_grouped.png
  - fig/comparison_by_difficulty.png
  - fig/comparison_by_location.png
Results saved: comparison_results.json
================================================================================
```

**Bi·ªÉu ƒë·ªì ƒë∆∞·ª£c t·∫°o:**

1. **comparison_grouped.png**
   - Hi·ªÉn th·ªã 3 metrics (AR, Delay, Throughput)
   - Nh√≥m theo `location_difficulty` (VD: atlanta_easy, chicago_medium)
   - So s√°nh DQN vs VAE-DQN cho t·ª´ng nh√≥m

2. **comparison_by_difficulty.png**
   - Hi·ªÉn th·ªã 3 metrics
   - Nh√≥m theo m·ª©c ƒë·ªô: Easy, Medium, Hard
   - Trung b√¨nh t·∫•t c·∫£ locations cho m·ªói m·ª©c ƒë·ªô

3. **comparison_by_location.png**
   - Hi·ªÉn th·ªã 3 metrics
   - Nh√≥m theo ƒë·ªãa danh (Atlanta, Chicago, Dallas, etc.)
   - Trung b√¨nh t·∫•t c·∫£ difficulties cho m·ªói location

#### Tr√™n file c·ª• th·ªÉ (Optional)
```bash
python main.py compare --data data/cogent_centers_atlanta_easy_s1.json --episodes 20
```

---

## üìä Chi ti·∫øt th√†nh ph·∫ßn

### `agents/`
**DRL agents implementation**

- **`dqn_agent.py`**: 
  - Deep Q-Network v·ªõi 3 inputs (DC, Demand, Global)
  - Experience replay buffer (50k transitions)
  - Target network update every 10k steps
  - Epsilon-greedy action selection
  
- **`vae_agent.py`**: 
  - VAE Encoder: DC_State ‚Üí 32D latent
  - VAE Decoder: Latent ‚Üí Next_DC_State
  - Value Network: Latent ‚Üí Priority score
  
- **`vae_trainer.py`**: 
  - Circular buffer for VAE data (50k samples)
  - Train VAE with reconstruction + KL loss
  - Train Value Network with MSE loss

### `core/`
**NFV Simulator business logic**

- **`dc.py`**: DataCenter (CPU/RAM/Storage) v√† SwitchNode
- **`request.py`**: SFC request v·ªõi VNF chain, bandwidth, deadline
- **`sfc_manager.py`**: Lifecycle management (active/completed/dropped)
- **`simulator.py`**: Discrete-event simulation, time advance
- **`topology.py`**: K-shortest paths, bandwidth allocation/release
- **`vnf.py`**: VNF instance (idle/busy state, processing time)
- **`statistics.py`**: Calculate acceptance ratio, delay, throughput

### `envs/`
**Gym Environment interface**

- **`env.py`**: Main SFCEnvironment class
  - `reset()`: Initialize episode
  - `step(action)`: Execute action, return (state, reward, done, info)
  
- **`action_handler.py`**: 
  - Execute ALLOCATE/UNINSTALL actions
  - Validate resources, bandwidth, latency
  - Calculate rewards
  
- **`observer.py`**: 
  - Construct state representation
  - Padding to MAX_VNF_TYPES=10
  
- **`selectors.py`**: 
  - PrioritySelector: Heuristic DC ordering
  - VAESelector: VAE value-based ordering
  - RandomSelector: Random ordering
  
- **`request_selector.py`**: Priority-based request selection
- **`utils.py`**: Action masking, type parsing

### `runners/`
**Training v√† testing pipelines**

- **`runner.py`**: Core Runner v·ªõi c√°c methods
- **`data_generator.py`**: Generate random scenarios
- **`data_loader.py`**: Load from JSON
- **`train_dqn.py`**: DQN training loop
- **`train_vae.py`**: VAE data collection + training
- **`compare.py`**: DQN vs VAE-DQN comparison v·ªõi grouped analysis

### `config.py`
**Global configuration**
```python
MAX_VNF_TYPES = 10              # Padding size
ACTION_SPACE_SIZE = 21          # 2*10 + 1
MAX_SIM_TIME_PER_EPISODE = 1000 # Max simulation time (ms)
LEARNING_RATE = 0.001
GAMMA = 0.95
EPSILON_START = 1.0
EPSILON_MIN = 0.01
BATCH_SIZE = 64
MEMORY_SIZE = 50000
```

## üìù Input Data Format

### Naming Convention
```
<location>_<difficulty>_s<scenario>.json
```

**Examples:**
- `cogent_centers_atlanta_easy_s1.json`
- `cogent_centers_chicago_medium_s2.json`
- `cogent_centers_dallas_hard_s3.json`

### File Structure
```json
{
  "V": {
    "0": {"server": true, "c_v": 100, "r_v": 200, "h_v": 150, "d_v": 0.1},
    "1": {"server": false}
  },
  "E": [
    {"u": 0, "v": 1, "b_l": 100, "d_l": 0.05}
  ],
  "F": [
    {"c_f": 1.2, "r_f": 1.0, "h_f": 0.8, "d_f": {"0": 0.3}}
  ],
  "R": [
    {"T": 1, "st_r": 0, "d_r": 1, "F_r": [0], "b_r": 1.5, "d_max": 50.0}
  ]
}
```

**Fields:**
- `V`: Nodes (DCs: `server=true`, Switches: `server=false`)
- `E`: Links v·ªõi bandwidth (`b_l`) v√† delay (`d_l`)
- `F`: VNF specifications (CPU, RAM, Storage requirements)
- `R`: Requests (arrival time, source, dest, VNF chain, bandwidth, max delay)

## üìà Performance Metrics

- **Acceptance Ratio**: `completed / (completed + dropped) √ó 100%`
- **Average E2E Delay**: Mean latency of completed requests
- **Throughput**: Total bandwidth of completed requests

## üîß Troubleshooting

### Training qu√° ch·∫≠m
```bash
# Gi·∫£m s·ªë episodes
python main.py train pipeline --episodes 200 --vae-episodes 100

# Ho·∫∑c gi·∫£m MAX_SIM_TIME_PER_EPISODE trong config.py
MAX_SIM_TIME_PER_EPISODE = 500
```

### Out of memory
```bash
# Gi·∫£m MEMORY_SIZE trong config.py
MEMORY_SIZE = 20000
```

### Model kh√¥ng converge
```bash
# TƒÉng s·ªë episodes
python main.py train pipeline --episodes 1000 --vae-episodes 300

# Ho·∫∑c ƒëi·ªÅu ch·ªânh learning rate trong config.py
LEARNING_RATE = 0.0005
```

### Missing plots
```bash
# C√†i ƒë·∫∑t matplotlib n·∫øu ch∆∞a c√≥
pip install matplotlib
```

## üéì References

Paper: "Unlocking Reconfigurability for Deep Reinforcement Learning in SFC Provisioning" (IEEE Networking Letters, 2024)

## üìß Contact

[Your contact info]