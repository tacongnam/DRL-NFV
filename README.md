# DRL-based Network Function Virtualization Placement

H·ªá th·ªëng s·ª≠ d·ª•ng Deep Reinforcement Learning (DQN + VAE) ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n ƒë·∫∑t Virtual Network Functions (VNFs) trong m√¥i tr∆∞·ªùng NFV/SDN.

## üìÅ C·∫•u tr√∫c th∆∞ m·ª•c
```
DRL-NFV/
‚îú‚îÄ‚îÄ agents/                          # C√°c agent DRL
‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.py                # Deep Q-Network agent
‚îÇ   ‚îú‚îÄ‚îÄ vae_agent.py                # Variational Autoencoder agent
‚îÇ   ‚îî‚îÄ‚îÄ vae_trainer.py              # Trainer cho VAE
‚îÇ
‚îú‚îÄ‚îÄ core/                            # Core logic c·ªßa simulator
‚îÇ   ‚îú‚îÄ‚îÄ dc.py                       # DataCenter v√† SwitchNode
‚îÇ   ‚îú‚îÄ‚îÄ request.py                  # SFC Request
‚îÇ   ‚îú‚îÄ‚îÄ sfc_manager.py              # Qu·∫£n l√Ω requests
‚îÇ   ‚îú‚îÄ‚îÄ simulator.py                # Discrete-event simulator
‚îÇ   ‚îú‚îÄ‚îÄ statistics.py               # Th·ªëng k√™ metrics
‚îÇ   ‚îú‚îÄ‚îÄ topology.py                 # Network topology manager
‚îÇ   ‚îî‚îÄ‚îÄ vnf.py                      # VNF Instance
‚îÇ
‚îú‚îÄ‚îÄ envs/                            # Gym Environment
‚îÇ   ‚îú‚îÄ‚îÄ action_handler.py           # X·ª≠ l√Ω actions (allocate/uninstall)
‚îÇ   ‚îú‚îÄ‚îÄ debug_tracker.py            # Debug v√† tracking
‚îÇ   ‚îú‚îÄ‚îÄ env.py                      # SFCEnvironment (Gym interface)
‚îÇ   ‚îú‚îÄ‚îÄ observer.py                 # State observation
‚îÇ   ‚îú‚îÄ‚îÄ request_selector.py         # Ch·ªçn request ∆∞u ti√™n
‚îÇ   ‚îú‚îÄ‚îÄ selectors.py                # DC ordering strategies
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                    # Action mask v√† utilities
‚îÇ
‚îú‚îÄ‚îÄ runners/                         # Training v√† testing pipelines
‚îÇ   ‚îú‚îÄ‚îÄ compare.py                  # So s√°nh DQN vs VAE-DQN
‚îÇ   ‚îú‚îÄ‚îÄ data_generator.py           # Generate random scenarios
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py              # Load t·ª´ JSON files
‚îÇ   ‚îú‚îÄ‚îÄ runner.py                   # Core Runner class
‚îÇ   ‚îú‚îÄ‚îÄ train_dqn.py                # Training DQN
‚îÇ   ‚îî‚îÄ‚îÄ train_vae.py                # Collect & train VAE
‚îÇ
‚îú‚îÄ‚îÄ data/                            # Test datasets (30 files)
‚îÇ   ‚îú‚îÄ‚îÄ scenario_001.json
‚îÇ   ‚îú‚îÄ‚îÄ scenario_002.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ models/                          # Saved models
‚îÇ   ‚îú‚îÄ‚îÄ best_model_q.weights.h5    # DQN model
‚îÇ   ‚îú‚îÄ‚îÄ vae_model_*.weights.h5      # VAE models
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint_*.weights.h5     # Checkpoints
‚îÇ
‚îú‚îÄ‚îÄ fig/                             # Output figures
‚îÇ   ‚îî‚îÄ‚îÄ comparison.png              # DQN vs VAE-DQN comparison
‚îÇ
‚îú‚îÄ‚îÄ config.py                        # Global configuration
‚îú‚îÄ‚îÄ main.py                          # CLI entry point
‚îî‚îÄ‚îÄ README.md
```

## üéØ √ù t∆∞·ªüng thu·∫≠t to√°n

### B√†i to√°n
ƒê·∫∑t c√°c Virtual Network Functions (VNFs) l√™n c√°c Data Centers ƒë·ªÉ ph·ª•c v·ª• Service Function Chain (SFC) requests v·ªõi m·ª•c ti√™u:
- **Maximize**: Acceptance Ratio (s·ªë requests ƒë∆∞·ª£c ph·ª•c v·ª•)
- **Minimize**: End-to-End Delay, Resource consumption
- **Constraints**: CPU, RAM, Storage, Bandwidth, Latency

### Ki·∫øn tr√∫c DRL

#### 1. Deep Q-Network (DQN)
- **State**: 3 inputs
  - DC State: `[CPU, RAM, installed_VNFs, idle_VNFs]`
  - DC-Demand State: `[VNF_demand, chain_patterns]`
  - Global State: `[total_requests, avg_delay, global_VNF_demand]`
  
- **Action Space**: `2V + 1` (V = MAX_VNF_TYPES = 10)
  - Action 0: WAIT
  - Actions 1‚Üí10: UNINSTALL VNF type 0-9
  - Actions 11‚Üí20: ALLOCATE VNF type 0-9

- **Reward**:
  - `+2.0`: SFC completed
  - `-1.5`: SFC dropped (timeout)
  - `-1.0`: Invalid action
  - `-0.5`: Uninstall needed VNF
  - `0.0`: Otherwise

- **Network Architecture**:
```
  Input1 [DC] ‚Üí Dense(32) ‚Üí
  Input2 [Demand] ‚Üí Dense(64) ‚Üí Concat ‚Üí Attention ‚Üí Dense(96) ‚Üí Dense(64) ‚Üí Q-values
  Input3 [Global] ‚Üí Dense(64) ‚Üí
```

#### 2. VAE-enhanced DQN
- **VAE Encoder**: DC_State ‚Üí Latent representation (32D)
- **VAE Decoder**: Latent ‚Üí Next_DC_State (prediction)
- **Value Network**: Latent ‚Üí DC priority score
- **Benefit**: DCs ƒë∆∞·ª£c s·∫Øp x·∫øp theo value t·ª´ VAE thay v√¨ heuristic priority

### Reconfigurability
- **Padding scheme**: State size c·ªë ƒë·ªãnh v·ªõi `MAX_VNF_TYPES=10`
- **Flexible training**: VNF types t·ª´ 2-10 trong m·ªói episode
- **No retraining needed**: Model ho·∫°t ƒë·ªông v·ªõi b·∫•t k·ª≥ s·ªë VNF types n√†o (2-10)

## üöÄ Pipeline ƒë·∫ßy ƒë·ªß

### Step 1: Train DQN (Random Data)
```bash
python main.py train random --episodes 500
```

**Ch·ª©c nƒÉng:**
- Generate random scenarios m·ªói episode v·ªõi progressive difficulty:
  - Episode 0-30%: DC=4-6, Nodes=10-16, Requests=15-30 (Easy)
  - Episode 30-60%: DC=5-8, Nodes=15-23, Requests=30-50 (Medium)
  - Episode 60-100%: DC=6-10, Nodes=16-30, Requests=40-80 (Hard)
- Epsilon decay: 1.0 ‚Üí 0.01
- Checkpoint every 50 episodes
- Save best model to `models/best_model`

**Output m·∫´u:**
```
Episode 1/500 [DC:4 N:14 VNF:6 Req:25]: R=450 AR=76.0% C:19 D:6 S:3245
Episode 2/500 [DC:6 N:18 VNF:8 Req:35]: R=623 AR=82.9% C:29 D:6 S:4521
...
Checkpoint 50: AR=85.34% R=712.3 Memory=45230
...
TRAINING COMPLETE: AR=94.28% R=1523.5
```

**Train v·ªõi file c·ª• th·ªÉ (optional):**
```bash
python main.py train dqn --data data/scenario_001.json --updates 40
```

---

### Step 2: Train VAE (Random Data)
```bash
python main.py train vae --episodes 200
```

**Ch·ª©c nƒÉng:**
- Collect DC state transitions t·ª´ random scenarios
- Train VAE Encoder + Decoder ƒë·ªÉ predict next DC state
- Train Value Network ƒë·ªÉ score DC priority
- Save to `models/vae_model`

**Output m·∫´u:**
```
Collecting VAE data: 200 episodes

Episode 10/200: 45230 samples
Episode 20/200: 89450 samples
...
Collected 234567 transitions

>>> Training VAE (234567 samples)...
    Epoch 5/50 - Loss: 0.4523
    Epoch 10/50 - Loss: 0.3241
    ...

>>> Training Value Network (234567 samples)...
    Epoch 5/100 - Loss: 0.2134
    Epoch 10/100 - Loss: 0.1567
    ...

‚úì VAE model saved to models/vae_model
```

**Train v·ªõi file c·ª• th·ªÉ (optional):**
```bash
python main.py train vae --data data/scenario_001.json --vae-episodes 100
```

---

### Step 3: Compare DQN vs VAE-DQN
```bash
python main.py compare
```

**Ch·ª©c nƒÉng:**
- Load c·∫£ 2 models (DQN v√† VAE-DQN)
- Test tr√™n **T·∫§T C·∫¢** files trong `data/` (30 files)
- So s√°nh performance: Acceptance Ratio, E2E Delay
- V·∫Ω bi·ªÉu ƒë·ªì comparison

**Output:**
```
================================================================================
Comparing DQN vs VAE-DQN on all files in data/
================================================================================

Testing 30 files...

File 1/30: scenario_001.json
  DQN:     AR=92.3% Delay=47.5ms
  VAE-DQN: AR=95.1% Delay=43.2ms

File 2/30: scenario_002.json
  DQN:     AR=89.7% Delay=51.3ms
  VAE-DQN: AR=93.4% Delay=46.8ms
...

================================================================================
OVERALL RESULTS (30 files)
================================================================================
DQN:
  Avg Acceptance Ratio: 90.45% ¬± 3.21%
  Avg E2E Delay: 48.32ms ¬± 5.67ms

VAE-DQN:
  Avg Acceptance Ratio: 93.78% ¬± 2.89%
  Avg E2E Delay: 44.15ms ¬± 4.23ms

Improvement:
  Acceptance Ratio: +3.33%
  E2E Delay: -8.63%

Plot saved: fig/comparison.png
Results saved: test_results.json
================================================================================
```

**Compare v·ªõi file c·ª• th·ªÉ (optional):**
```bash
python main.py compare --data data/scenario_001.json --episodes 20
```

---

## üìä Chi ti·∫øt th√†nh ph·∫ßn

### `agents/`
**DRL agents implementation**

- **`dqn_agent.py`**: 
  - Deep Q-Network v·ªõi 3 inputs (DC, Demand, Global)
  - Experience replay buffer (50k transitions)
  - Target network update every 10k steps
  - Epsilon-greedy action selection
  
- **`vae_agent.py`**: 
  - VAE Encoder: DC_State ‚Üí 32D latent
  - VAE Decoder: Latent ‚Üí Next_DC_State
  - Value Network: Latent ‚Üí Priority score
  
- **`vae_trainer.py`**: 
  - Circular buffer for VAE data (50k samples)
  - Train VAE with reconstruction + KL loss
  - Train Value Network with MSE loss

### `core/`
**NFV Simulator business logic**

- **`dc.py`**: DataCenter (CPU/RAM/Storage) v√† SwitchNode
- **`request.py`**: SFC request v·ªõi VNF chain, bandwidth, deadline
- **`sfc_manager.py`**: Lifecycle management (active/completed/dropped)
- **`simulator.py`**: Discrete-event simulation, time advance
- **`topology.py`**: K-shortest paths, bandwidth allocation/release
- **`vnf.py`**: VNF instance (idle/busy state, processing time)
- **`statistics.py`**: Calculate acceptance ratio, delay, throughput

### `envs/`
**Gym Environment interface**

- **`env.py`**: Main SFCEnvironment class
  - `reset()`: Initialize episode
  - `step(action)`: Execute action, return (state, reward, done, info)
  
- **`action_handler.py`**: 
  - Execute ALLOCATE/UNINSTALL actions
  - Validate resources, bandwidth, latency
  - Calculate rewards
  
- **`observer.py`**: 
  - Construct state representation
  - Padding to MAX_VNF_TYPES=10
  
- **`selectors.py`**: 
  - PrioritySelector: Heuristic DC ordering
  - VAESelector: VAE value-based ordering
  - RandomSelector: Random ordering
  
- **`request_selector.py`**: Priority-based request selection
- **`utils.py`**: Action masking, type parsing

### `runners/`
**Training v√† testing pipelines**

- **`runner.py`**: Core Runner v·ªõi c√°c methods
- **`data_generator.py`**: Generate random scenarios
- **`data_loader.py`**: Load from JSON
- **`train_dqn.py`**: DQN training loop
- **`train_vae.py`**: VAE data collection + training
- **`compare.py`**: DQN vs VAE-DQN comparison

### `config.py`
**Global configuration**
```python
MAX_VNF_TYPES = 10              # Padding size
ACTION_SPACE_SIZE = 21          # 2*10 + 1
MAX_SIM_TIME_PER_EPISODE = 5000 # Max simulation time (ms)
LEARNING_RATE = 0.001
GAMMA = 0.95
EPSILON_START = 1.0
EPSILON_MIN = 0.01
BATCH_SIZE = 64
MEMORY_SIZE = 50000
```

## üìù Input Data Format
```json
{
  "V": {
    "0": {"server": true, "c_v": 100, "r_v": 200, "h_v": 150, "d_v": 0.1},
    "1": {"server": false}
  },
  "E": [
    {"u": 0, "v": 1, "b_l": 100, "d_l": 0.05}
  ],
  "F": [
    {"c_f": 1.2, "r_f": 1.0, "h_f": 0.8, "d_f": {"0": 0.3}}
  ],
  "R": [
    {"T": 1, "st_r": 0, "d_r": 1, "F_r": [0], "b_r": 1.5, "d_max": 50.0}
  ]
}
```

**Fields:**
- `V`: Nodes (DCs: `server=true`, Switches: `server=false`)
- `E`: Links v·ªõi bandwidth (`b_l`) v√† delay (`d_l`)
- `F`: VNF specifications (CPU, RAM, Storage requirements)
- `R`: Requests (arrival time, source, dest, VNF chain, bandwidth, max delay)

## üìà Performance Metrics

- **Acceptance Ratio**: `completed / (completed + dropped) √ó 100%`
- **Average E2E Delay**: Mean latency of completed requests
- **Throughput**: Total bandwidth of completed requests

## üîß Troubleshooting

### Training qu√° ch·∫≠m
```bash
# Gi·∫£m s·ªë episodes
python main.py train random --episodes 200

# Ho·∫∑c gi·∫£m MAX_SIM_TIME_PER_EPISODE trong config.py
MAX_SIM_TIME_PER_EPISODE = 3000
```

### Out of memory
```bash
# Gi·∫£m MEMORY_SIZE trong config.py
MEMORY_SIZE = 20000
```

### Model kh√¥ng converge
```bash
# TƒÉng s·ªë episodes
python main.py train random --episodes 1000

# Ho·∫∑c ƒëi·ªÅu ch·ªânh learning rate trong config.py
LEARNING_RATE = 0.0005
```

## üéì References

Paper: "Unlocking Reconfigurability for Deep Reinforcement Learning in SFC Provisioning" (IEEE Networking Letters, 2024)

## üìß Contact

[Your contact info]